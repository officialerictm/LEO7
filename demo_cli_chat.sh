#!/usr/bin/env bash
# Demo: Leonardo CLI Chat with Ollama Integration

echo "Leonardo AI CLI Chat - Now with Ollama Integration!"
echo "==================================================="
echo
echo "The CLI chat interface is now fully connected to Ollama for AI responses."
echo
echo "Key improvements:"
echo "âœ“ Automatic model detection and download"
echo "âœ“ Real-time response generation"
echo "âœ“ Fallback to llama.cpp if Ollama not available"
echo "âœ“ Better error handling"
echo
echo "To start chatting with AI:"
echo "  1. Make sure Ollama is running: ollama serve"
echo "  2. Run: ./leonardo.sh chat"
echo "  3. Or specify a model: ./leonardo.sh chat llama3.2:1b"
echo
echo "The chat interface will:"
echo "- Automatically pull the model if not available"
echo "- Stream responses in real-time"
echo "- Support conversation history"
echo "- Allow saving conversations"
echo
echo "Chat commands:"
echo "  /exit or /quit - End chat"
echo "  /clear - Clear conversation"
echo "  /save [filename] - Save conversation"
echo "  /help - Show help"
echo
echo "Happy chatting! ðŸ¤–"
